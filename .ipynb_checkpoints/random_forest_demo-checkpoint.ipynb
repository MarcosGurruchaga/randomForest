{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo del Algoritmo Random Forest\n",
        "\n",
        "Este notebook presenta una demostraci√≥n completa del algoritmo Random Forest, explicando sus hiperpar√°metros principales y c√≥mo afectan al rendimiento del modelo.\n",
        "\n",
        "## ¬øQu√© es Random Forest?\n",
        "\n",
        "Random Forest es un algoritmo de aprendizaje autom√°tico que combina m√∫ltiples √°rboles de decisi√≥n para crear un modelo m√°s robusto y preciso. Es un m√©todo de **ensemble learning** que utiliza la t√©cnica de **bagging** (Bootstrap Aggregating).\n",
        "\n",
        "### Ventajas de Random Forest:\n",
        "- Reduce el overfitting\n",
        "- Maneja bien datos faltantes\n",
        "- Proporciona importancia de caracter√≠sticas\n",
        "- Es robusto a outliers\n",
        "- Funciona bien con datos categ√≥ricos y num√©ricos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hiperpar√°metros Principales de Random Forest\n",
        "\n",
        "### 1. **n_estimators** (n√∫mero de √°rboles)\n",
        "- **Qu√© hace**: Define cu√°ntos √°rboles de decisi√≥n se crear√°n en el bosque\n",
        "- **Valor por defecto**: 100\n",
        "- **Efecto**: M√°s √°rboles generalmente mejoran la precisi√≥n pero aumentan el tiempo de entrenamiento\n",
        "\n",
        "### 2. **max_depth** (profundidad m√°xima)\n",
        "- **Qu√© hace**: Controla la profundidad m√°xima de cada √°rbol\n",
        "- **Valor por defecto**: None (sin l√≠mite)\n",
        "- **Efecto**: Limita el crecimiento del √°rbol para evitar overfitting\n",
        "\n",
        "### 3. **min_samples_split** (m√≠nimo de muestras para dividir)\n",
        "- **Qu√© hace**: N√∫mero m√≠nimo de muestras requeridas para dividir un nodo interno\n",
        "- **Valor por defecto**: 2\n",
        "- **Efecto**: Valores m√°s altos previenen overfitting\n",
        "\n",
        "### 4. **min_samples_leaf** (m√≠nimo de muestras en hoja)\n",
        "- **Qu√© hace**: N√∫mero m√≠nimo de muestras requeridas en un nodo hoja\n",
        "- **Valor por defecto**: 1\n",
        "- **Efecto**: Valores m√°s altos suavizan el modelo\n",
        "\n",
        "### 5. **max_features** (caracter√≠sticas m√°ximas)\n",
        "- **Qu√© hace**: N√∫mero de caracter√≠sticas a considerar al buscar la mejor divisi√≥n\n",
        "- **Valores comunes**: 'sqrt', 'log2', None, o un n√∫mero espec√≠fico\n",
        "- **Efecto**: Controla la aleatoriedad y puede reducir overfitting\n",
        "\n",
        "### 6. **bootstrap** (muestreo con reemplazo)\n",
        "- **Qu√© hace**: Si usar muestreo bootstrap al construir √°rboles\n",
        "- **Valor por defecto**: True\n",
        "- **Efecto**: True permite diversidad entre √°rboles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar las librer√≠as necesarias\n",
        "import numpy as np  # Para operaciones num√©ricas y arrays\n",
        "import pandas as pd  # Para manipulaci√≥n de datos estructurados\n",
        "import matplotlib.pyplot as plt  # Para crear gr√°ficos y visualizaciones\n",
        "import seaborn as sns  # Para gr√°ficos estad√≠sticos m√°s avanzados\n",
        "from sklearn.datasets import make_classification  # Para generar datos sint√©ticos de clasificaci√≥n\n",
        "from sklearn.model_selection import train_test_split  # Para dividir datos en entrenamiento y prueba\n",
        "from sklearn.ensemble import RandomForestClassifier  # El algoritmo Random Forest\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # M√©tricas de evaluaci√≥n\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score  # M√©tricas adicionales\n",
        "\n",
        "# Configurar el estilo de los gr√°ficos para que se vean mejor\n",
        "plt.style.use('seaborn-v0_8')  # Usar estilo seaborn para gr√°ficos m√°s atractivos\n",
        "sns.set_palette(\"husl\")  # Configurar paleta de colores para los gr√°ficos\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar un dataset sint√©tico para la demostraci√≥n\n",
        "# make_classification crea un dataset de clasificaci√≥n con caracter√≠sticas controladas\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,  # N√∫mero total de muestras (filas)\n",
        "    n_features=10,   # N√∫mero de caracter√≠sticas (columnas)\n",
        "    n_informative=8, # N√∫mero de caracter√≠sticas informativas (que realmente ayudan a clasificar)\n",
        "    n_redundant=2,   # N√∫mero de caracter√≠sticas redundantes (correlacionadas con las informativas)\n",
        "    n_classes=3,     # N√∫mero de clases diferentes\n",
        "    random_state=42  # Semilla para reproducibilidad\n",
        ")\n",
        "\n",
        "# Convertir a DataFrame para mejor visualizaci√≥n\n",
        "df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(X.shape[1])])\n",
        "df['target'] = y  # Agregar la variable objetivo\n",
        "\n",
        "print(\"üìä Informaci√≥n del Dataset:\")\n",
        "print(f\"Forma del dataset: {X.shape}\")\n",
        "print(f\"N√∫mero de clases: {len(np.unique(y))}\")\n",
        "print(f\"Distribuci√≥n de clases: {np.bincount(y)}\")\n",
        "print(\"\\nüîç Primeras 5 filas del dataset:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
        "# train_test_split separa los datos de manera aleatoria pero reproducible\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,                    # Datos de entrada y salida\n",
        "    test_size=0.2,           # 20% de los datos para prueba, 80% para entrenamiento\n",
        "    random_state=42,         # Semilla para reproducibilidad\n",
        "    stratify=y               # Mantener la proporci√≥n de clases en ambos conjuntos\n",
        ")\n",
        "\n",
        "print(\"üìà Divisi√≥n de datos:\")\n",
        "print(f\"Conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
        "print(f\"Conjunto de prueba: {X_test.shape[0]} muestras\")\n",
        "print(f\"Proporci√≥n entrenamiento/prueba: {X_train.shape[0]/X_test.shape[0]:.1f}:1\")\n",
        "\n",
        "# Verificar que la distribuci√≥n de clases se mantiene\n",
        "print(f\"\\nDistribuci√≥n de clases en entrenamiento: {np.bincount(y_train)}\")\n",
        "print(f\"Distribuci√≥n de clases en prueba: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demostraci√≥n de Hiperpar√°metros\n",
        "\n",
        "Ahora vamos a entrenar diferentes modelos Random Forest con diferentes configuraciones de hiperpar√°metros para ver c√≥mo afectan al rendimiento del modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Random Forest con par√°metros por defecto\n",
        "print(\"üå≤ Modelo 1: Random Forest con par√°metros por defecto\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Crear el modelo con par√°metros por defecto\n",
        "rf_default = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "rf_default.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred_default = rf_default.predict(X_test)\n",
        "\n",
        "# Calcular m√©tricas de rendimiento\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "precision_default = precision_score(y_test, y_pred_default, average='weighted')\n",
        "recall_default = recall_score(y_test, y_pred_default, average='weighted')\n",
        "f1_default = f1_score(y_test, y_pred_default, average='weighted')\n",
        "\n",
        "print(f\"üìä M√©tricas del modelo por defecto:\")\n",
        "print(f\"   Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Precision: {precision_default:.4f}\")\n",
        "print(f\"   Recall: {recall_default:.4f}\")\n",
        "print(f\"   F1-Score: {f1_default:.4f}\")\n",
        "\n",
        "# Mostrar los par√°metros utilizados\n",
        "print(f\"\\n‚öôÔ∏è Par√°metros utilizados:\")\n",
        "print(f\"   n_estimators: {rf_default.n_estimators}\")\n",
        "print(f\"   max_depth: {rf_default.max_depth}\")\n",
        "print(f\"   min_samples_split: {rf_default.min_samples_split}\")\n",
        "print(f\"   min_samples_leaf: {rf_default.min_samples_leaf}\")\n",
        "print(f\"   max_features: {rf_default.max_features}\")\n",
        "print(f\"   bootstrap: {rf_default.bootstrap}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Random Forest con m√°s √°rboles (n_estimators)\n",
        "print(\"\\nüå≤ Modelo 2: Random Forest con m√°s √°rboles (n_estimators=200)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Crear el modelo con m√°s √°rboles\n",
        "rf_more_trees = RandomForestClassifier(\n",
        "    n_estimators=200,  # Aumentar de 100 a 200 √°rboles\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "rf_more_trees.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_more_trees = rf_more_trees.predict(X_test)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "accuracy_more_trees = accuracy_score(y_test, y_pred_more_trees)\n",
        "precision_more_trees = precision_score(y_test, y_pred_more_trees, average='weighted')\n",
        "recall_more_trees = recall_score(y_test, y_pred_more_trees, average='weighted')\n",
        "f1_more_trees = f1_score(y_test, y_pred_more_trees, average='weighted')\n",
        "\n",
        "print(f\"üìä M√©tricas del modelo con m√°s √°rboles:\")\n",
        "print(f\"   Accuracy: {accuracy_more_trees:.4f}\")\n",
        "print(f\"   Precision: {precision_more_trees:.4f}\")\n",
        "print(f\"   Recall: {recall_more_trees:.4f}\")\n",
        "print(f\"   F1-Score: {f1_more_trees:.4f}\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Par√°metros utilizados:\")\n",
        "print(f\"   n_estimators: {rf_more_trees.n_estimators}\")\n",
        "print(f\"   max_depth: {rf_more_trees.max_depth}\")\n",
        "print(f\"   min_samples_split: {rf_more_trees.min_samples_split}\")\n",
        "print(f\"   min_samples_leaf: {rf_more_trees.min_samples_leaf}\")\n",
        "print(f\"   max_features: {rf_more_trees.max_features}\")\n",
        "print(f\"   bootstrap: {rf_more_trees.bootstrap}\")\n",
        "\n",
        "# Comparar con el modelo anterior\n",
        "print(f\"\\nüìà Comparaci√≥n con modelo por defecto:\")\n",
        "print(f\"   Mejora en Accuracy: {accuracy_more_trees - accuracy_default:.4f}\")\n",
        "print(f\"   Mejora en F1-Score: {f1_more_trees - f1_default:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 3: Random Forest con profundidad limitada (max_depth)\n",
        "print(\"\\nüå≤ Modelo 3: Random Forest con profundidad limitada (max_depth=5)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Crear el modelo con profundidad limitada\n",
        "rf_limited_depth = RandomForestClassifier(\n",
        "    n_estimators=100,     # Mantener 100 √°rboles\n",
        "    max_depth=5,          # Limitar la profundidad m√°xima a 5 niveles\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "rf_limited_depth.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_limited_depth = rf_limited_depth.predict(X_test)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "accuracy_limited_depth = accuracy_score(y_test, y_pred_limited_depth)\n",
        "precision_limited_depth = precision_score(y_test, y_pred_limited_depth, average='weighted')\n",
        "recall_limited_depth = recall_score(y_test, y_pred_limited_depth, average='weighted')\n",
        "f1_limited_depth = f1_score(y_test, y_pred_limited_depth, average='weighted')\n",
        "\n",
        "print(f\"üìä M√©tricas del modelo con profundidad limitada:\")\n",
        "print(f\"   Accuracy: {accuracy_limited_depth:.4f}\")\n",
        "print(f\"   Precision: {precision_limited_depth:.4f}\")\n",
        "print(f\"   Recall: {recall_limited_depth:.4f}\")\n",
        "print(f\"   F1-Score: {f1_limited_depth:.4f}\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Par√°metros utilizados:\")\n",
        "print(f\"   n_estimators: {rf_limited_depth.n_estimators}\")\n",
        "print(f\"   max_depth: {rf_limited_depth.max_depth}\")\n",
        "print(f\"   min_samples_split: {rf_limited_depth.min_samples_split}\")\n",
        "print(f\"   min_samples_leaf: {rf_limited_depth.min_samples_leaf}\")\n",
        "print(f\"   max_features: {rf_limited_depth.max_features}\")\n",
        "print(f\"   bootstrap: {rf_limited_depth.bootstrap}\")\n",
        "\n",
        "# Comparar con el modelo por defecto\n",
        "print(f\"\\nüìà Comparaci√≥n con modelo por defecto:\")\n",
        "print(f\"   Cambio en Accuracy: {accuracy_limited_depth - accuracy_default:.4f}\")\n",
        "print(f\"   Cambio en F1-Score: {f1_limited_depth - f1_default:.4f}\")\n",
        "\n",
        "print(f\"\\nüí° Interpretaci√≥n:\")\n",
        "print(f\"   Limitar la profundidad puede prevenir overfitting\")\n",
        "print(f\"   pero tambi√©n puede reducir la capacidad del modelo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 4: Random Forest con par√°metros m√°s restrictivos\n",
        "print(\"\\nüå≤ Modelo 4: Random Forest con par√°metros m√°s restrictivos\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Crear el modelo con par√°metros m√°s restrictivos para prevenir overfitting\n",
        "rf_restrictive = RandomForestClassifier(\n",
        "    n_estimators=100,           # 100 √°rboles\n",
        "    max_depth=3,                # Profundidad muy limitada\n",
        "    min_samples_split=10,       # Requerir al menos 10 muestras para dividir\n",
        "    min_samples_leaf=5,         # Requerir al menos 5 muestras en cada hoja\n",
        "    max_features='sqrt',        # Usar solo sqrt(n_features) caracter√≠sticas por divisi√≥n\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "rf_restrictive.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred_restrictive = rf_restrictive.predict(X_test)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "accuracy_restrictive = accuracy_score(y_test, y_pred_restrictive)\n",
        "precision_restrictive = precision_score(y_test, y_pred_restrictive, average='weighted')\n",
        "recall_restrictive = recall_score(y_test, y_pred_restrictive, average='weighted')\n",
        "f1_restrictive = f1_score(y_test, y_pred_restrictive, average='weighted')\n",
        "\n",
        "print(f\"üìä M√©tricas del modelo restrictivo:\")\n",
        "print(f\"   Accuracy: {accuracy_restrictive:.4f}\")\n",
        "print(f\"   Precision: {precision_restrictive:.4f}\")\n",
        "print(f\"   Recall: {recall_restrictive:.4f}\")\n",
        "print(f\"   F1-Score: {f1_restrictive:.4f}\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Par√°metros utilizados:\")\n",
        "print(f\"   n_estimators: {rf_restrictive.n_estimators}\")\n",
        "print(f\"   max_depth: {rf_restrictive.max_depth}\")\n",
        "print(f\"   min_samples_split: {rf_restrictive.min_samples_split}\")\n",
        "print(f\"   min_samples_leaf: {rf_restrictive.min_samples_leaf}\")\n",
        "print(f\"   max_features: {rf_restrictive.max_features}\")\n",
        "print(f\"   bootstrap: {rf_restrictive.bootstrap}\")\n",
        "\n",
        "# Comparar con el modelo por defecto\n",
        "print(f\"\\nüìà Comparaci√≥n con modelo por defecto:\")\n",
        "print(f\"   Cambio en Accuracy: {accuracy_restrictive - accuracy_default:.4f}\")\n",
        "print(f\"   Cambio en F1-Score: {f1_restrictive - f1_default:.4f}\")\n",
        "\n",
        "print(f\"\\nüí° Interpretaci√≥n:\")\n",
        "print(f\"   Par√°metros restrictivos previenen overfitting\")\n",
        "print(f\"   pero pueden hacer el modelo demasiado simple\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumen comparativo de todos los modelos\n",
        "print(\"üìä RESUMEN COMPARATIVO DE TODOS LOS MODELOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Crear un DataFrame con los resultados\n",
        "results = pd.DataFrame({\n",
        "    'Modelo': ['Por Defecto', 'M√°s √Årboles', 'Profundidad Limitada', 'Restrictivo'],\n",
        "    'Accuracy': [accuracy_default, accuracy_more_trees, accuracy_limited_depth, accuracy_restrictive],\n",
        "    'Precision': [precision_default, precision_more_trees, precision_limited_depth, precision_restrictive],\n",
        "    'Recall': [recall_default, recall_more_trees, recall_limited_depth, recall_restrictive],\n",
        "    'F1-Score': [f1_default, f1_more_trees, f1_limited_depth, f1_restrictive]\n",
        "})\n",
        "\n",
        "# Mostrar el DataFrame con formato\n",
        "print(results.round(4))\n",
        "\n",
        "# Encontrar el mejor modelo\n",
        "best_model_idx = results['F1-Score'].idxmax()\n",
        "best_model_name = results.loc[best_model_idx, 'Modelo']\n",
        "best_f1_score = results.loc[best_model_idx, 'F1-Score']\n",
        "\n",
        "print(f\"\\nüèÜ Mejor modelo: {best_model_name}\")\n",
        "print(f\"   F1-Score: {best_f1_score:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà An√°lisis de resultados:\")\n",
        "print(f\"   ‚Ä¢ M√°s √°rboles pueden mejorar la precisi√≥n\")\n",
        "print(f\"   ‚Ä¢ Limitar profundidad puede prevenir overfitting\")\n",
        "print(f\"   ‚Ä¢ Par√°metros restrictivos pueden ser demasiado conservadores\")\n",
        "print(f\"   ‚Ä¢ El equilibrio entre complejidad y generalizaci√≥n es clave\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de los resultados\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))  # Crear una figura con 4 subplots\n",
        "fig.suptitle('Comparaci√≥n de Modelos Random Forest', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Gr√°fico 1: Comparaci√≥n de Accuracy\n",
        "axes[0, 0].bar(results['Modelo'], results['Accuracy'], color='skyblue', alpha=0.7)\n",
        "axes[0, 0].set_title('Accuracy por Modelo', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)  # Rotar etiquetas del eje x para mejor legibilidad\n",
        "axes[0, 0].grid(True, alpha=0.3)  # Agregar cuadr√≠cula sutil\n",
        "\n",
        "# Gr√°fico 2: Comparaci√≥n de F1-Score\n",
        "axes[0, 1].bar(results['Modelo'], results['F1-Score'], color='lightgreen', alpha=0.7)\n",
        "axes[0, 1].set_title('F1-Score por Modelo', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('F1-Score')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 3: Comparaci√≥n de Precision y Recall\n",
        "x_pos = np.arange(len(results['Modelo']))  # Posiciones en el eje x\n",
        "width = 0.35  # Ancho de las barras\n",
        "\n",
        "axes[1, 0].bar(x_pos - width/2, results['Precision'], width, label='Precision', alpha=0.7)\n",
        "axes[1, 0].bar(x_pos + width/2, results['Recall'], width, label='Recall', alpha=0.7)\n",
        "axes[1, 0].set_title('Precision vs Recall', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(results['Modelo'], rotation=45)\n",
        "axes[1, 0].legend()  # Mostrar leyenda\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 4: Heatmap de todas las m√©tricas\n",
        "metrics_data = results[['Accuracy', 'Precision', 'Recall', 'F1-Score']].T  # Transponer para el heatmap\n",
        "im = axes[1, 1].imshow(metrics_data.values, cmap='YlOrRd', aspect='auto')\n",
        "axes[1, 1].set_title('Heatmap de M√©tricas', fontweight='bold')\n",
        "axes[1, 1].set_xticks(range(len(results['Modelo'])))\n",
        "axes[1, 1].set_xticklabels(results['Modelo'], rotation=45)\n",
        "axes[1, 1].set_yticks(range(len(metrics_data.index)))\n",
        "axes[1, 1].set_yticklabels(metrics_data.index)\n",
        "\n",
        "# Agregar valores num√©ricos en el heatmap\n",
        "for i in range(len(metrics_data.index)):\n",
        "    for j in range(len(results['Modelo'])):\n",
        "        text = axes[1, 1].text(j, i, f'{metrics_data.iloc[i, j]:.3f}',\n",
        "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "plt.tight_layout()  # Ajustar el espaciado entre subplots\n",
        "plt.show()  # Mostrar el gr√°fico\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de importancia de caracter√≠sticas\n",
        "print(\"üîç AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Usar el mejor modelo para analizar la importancia de caracter√≠sticas\n",
        "best_model = rf_more_trees  # Usar el modelo con m√°s √°rboles como ejemplo\n",
        "\n",
        "# Obtener la importancia de caracter√≠sticas\n",
        "feature_importance = best_model.feature_importances_\n",
        "\n",
        "# Crear un DataFrame con la importancia de caracter√≠sticas\n",
        "feature_names = [f'Feature_{i+1}' for i in range(len(feature_importance))]\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)  # Ordenar por importancia descendente\n",
        "\n",
        "print(\"üìä Importancia de caracter√≠sticas (ordenadas por importancia):\")\n",
        "print(importance_df.round(4))\n",
        "\n",
        "# Visualizar la importancia de caracter√≠sticas\n",
        "plt.figure(figsize=(12, 8))  # Crear una nueva figura con tama√±o espec√≠fico\n",
        "bars = plt.bar(range(len(importance_df)), importance_df['Importance'], \n",
        "               color='coral', alpha=0.7)  # Crear barras con color coral\n",
        "plt.title('Importancia de Caracter√≠sticas en Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Caracter√≠sticas', fontsize=12)\n",
        "plt.ylabel('Importancia', fontsize=12)\n",
        "plt.xticks(range(len(importance_df)), importance_df['Feature'], rotation=45)  # Etiquetas del eje x\n",
        "plt.grid(True, alpha=0.3)  # Agregar cuadr√≠cula\n",
        "\n",
        "# Agregar valores num√©ricos en las barras\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()  # Ajustar el espaciado\n",
        "plt.show()  # Mostrar el gr√°fico\n",
        "\n",
        "print(f\"\\nüí° Interpretaci√≥n:\")\n",
        "print(f\"   ‚Ä¢ Las caracter√≠sticas con mayor importancia contribuyen m√°s a las predicciones\")\n",
        "print(f\"   ‚Ä¢ Feature_{importance_df.iloc[0]['Feature'].split('_')[1]} es la m√°s importante\")\n",
        "print(f\"   ‚Ä¢ Las caracter√≠sticas con baja importancia podr√≠an ser eliminadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matriz de confusi√≥n para el mejor modelo\n",
        "print(\"üìä MATRIZ DE CONFUSI√ìN DEL MEJOR MODELO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calcular la matriz de confusi√≥n para el mejor modelo\n",
        "cm = confusion_matrix(y_test, y_pred_more_trees)\n",
        "\n",
        "# Crear visualizaci√≥n de la matriz de confusi√≥n\n",
        "plt.figure(figsize=(8, 6))  # Crear figura con tama√±o espec√≠fico\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  # Crear heatmap con anotaciones\n",
        "            xticklabels=[f'Clase {i}' for i in range(len(np.unique(y)))],\n",
        "            yticklabels=[f'Clase {i}' for i in range(len(np.unique(y)))])\n",
        "plt.title('Matriz de Confusi√≥n - Modelo con M√°s √Årboles', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicci√≥n', fontsize=12)\n",
        "plt.ylabel('Valor Real', fontsize=12)\n",
        "plt.show()  # Mostrar el gr√°fico\n",
        "\n",
        "# Mostrar reporte de clasificaci√≥n detallado\n",
        "print(\"\\nüìã Reporte de Clasificaci√≥n Detallado:\")\n",
        "print(classification_report(y_test, y_pred_more_trees, \n",
        "                          target_names=[f'Clase {i}' for i in range(len(np.unique(y)))]))\n",
        "\n",
        "# An√°lisis de la matriz de confusi√≥n\n",
        "print(f\"\\nüîç An√°lisis de la Matriz de Confusi√≥n:\")\n",
        "print(f\"   ‚Ä¢ Total de predicciones correctas: {np.trace(cm)}\")\n",
        "print(f\"   ‚Ä¢ Total de predicciones: {np.sum(cm)}\")\n",
        "print(f\"   ‚Ä¢ Accuracy calculada: {np.trace(cm) / np.sum(cm):.4f}\")\n",
        "\n",
        "# Calcular precision, recall y F1 para cada clase\n",
        "for i in range(len(np.unique(y))):\n",
        "    tp = cm[i, i]  # Verdaderos positivos\n",
        "    fp = cm[:, i].sum() - tp  # Falsos positivos\n",
        "    fn = cm[i, :].sum() - tp  # Falsos negativos\n",
        "    \n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    print(f\"   ‚Ä¢ Clase {i}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones y Recomendaciones\n",
        "\n",
        "### üìä Resumen de la Demostraci√≥n\n",
        "\n",
        "En esta demostraci√≥n hemos visto c√≥mo diferentes hiperpar√°metros afectan el rendimiento del algoritmo Random Forest:\n",
        "\n",
        "1. **n_estimators**: M√°s √°rboles generalmente mejoran la precisi√≥n pero aumentan el tiempo de entrenamiento\n",
        "2. **max_depth**: Limitar la profundidad previene overfitting pero puede reducir la capacidad del modelo\n",
        "3. **min_samples_split/min_samples_leaf**: Par√°metros restrictivos previenen overfitting pero pueden hacer el modelo demasiado simple\n",
        "4. **max_features**: Controla la aleatoriedad y puede mejorar la generalizaci√≥n\n",
        "\n",
        "### üéØ Recomendaciones Pr√°cticas\n",
        "\n",
        "#### Para Datasets Peque√±os:\n",
        "- Usar `max_depth` limitado (3-5)\n",
        "- Aumentar `min_samples_split` y `min_samples_leaf`\n",
        "- Usar `max_features='sqrt'` o `max_features='log2'`\n",
        "\n",
        "#### Para Datasets Grandes:\n",
        "- Usar m√°s √°rboles (`n_estimators=200-500`)\n",
        "- Permitir mayor profundidad o usar `max_depth=None`\n",
        "- Usar `max_features='sqrt'` para balancear velocidad y precisi√≥n\n",
        "\n",
        "#### Para Prevenir Overfitting:\n",
        "- Limitar `max_depth`\n",
        "- Aumentar `min_samples_split` y `min_samples_leaf`\n",
        "- Usar `max_features` menor que el total de caracter√≠sticas\n",
        "\n",
        "#### Para Mejorar Precisi√≥n:\n",
        "- Aumentar `n_estimators`\n",
        "- Permitir mayor profundidad\n",
        "- Usar `max_features=None` (todas las caracter√≠sticas)\n",
        "\n",
        "### üîß Proceso de Optimizaci√≥n\n",
        "\n",
        "1. **Empezar con par√°metros por defecto**\n",
        "2. **Identificar si hay overfitting o underfitting**\n",
        "3. **Ajustar par√°metros gradualmente**\n",
        "4. **Usar validaci√≥n cruzada para evaluar cambios**\n",
        "5. **Considerar el balance entre precisi√≥n y tiempo de entrenamiento**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demostraci√≥n adicional: Efecto del n√∫mero de √°rboles en el rendimiento\n",
        "print(\"üå≤ AN√ÅLISIS DEL EFECTO DEL N√öMERO DE √ÅRBOLES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Probar diferentes n√∫meros de √°rboles\n",
        "n_trees_range = [10, 25, 50, 100, 200, 300]  # Rango de √°rboles a probar\n",
        "accuracies = []  # Lista para almacenar accuracies\n",
        "training_times = []  # Lista para almacenar tiempos de entrenamiento\n",
        "\n",
        "print(\"üîÑ Entrenando modelos con diferentes n√∫meros de √°rboles...\")\n",
        "\n",
        "for n_trees in n_trees_range:\n",
        "    # Crear modelo con n√∫mero espec√≠fico de √°rboles\n",
        "    rf_temp = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    \n",
        "    # Medir tiempo de entrenamiento\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    rf_temp.fit(X_train, y_train)  # Entrenar el modelo\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Hacer predicciones y calcular accuracy\n",
        "    y_pred_temp = rf_temp.predict(X_test)\n",
        "    accuracy_temp = accuracy_score(y_test, y_pred_temp)\n",
        "    \n",
        "    # Almacenar resultados\n",
        "    accuracies.append(accuracy_temp)\n",
        "    training_times.append(training_time)\n",
        "    \n",
        "    print(f\"   {n_trees:3d} √°rboles: Accuracy={accuracy_temp:.4f}, Tiempo={training_time:.3f}s\")\n",
        "\n",
        "# Crear visualizaci√≥n del efecto del n√∫mero de √°rboles\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))  # Crear figura con 2 subplots lado a lado\n",
        "\n",
        "# Gr√°fico 1: Accuracy vs N√∫mero de √°rboles\n",
        "ax1.plot(n_trees_range, accuracies, 'o-', linewidth=2, markersize=8, color='blue')\n",
        "ax1.set_title('Accuracy vs N√∫mero de √Årboles', fontweight='bold')\n",
        "ax1.set_xlabel('N√∫mero de √Årboles')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(min(accuracies) - 0.01, max(accuracies) + 0.01)  # Ajustar l√≠mites del eje y\n",
        "\n",
        "# Gr√°fico 2: Tiempo de entrenamiento vs N√∫mero de √°rboles\n",
        "ax2.plot(n_trees_range, training_times, 'o-', linewidth=2, markersize=8, color='red')\n",
        "ax2.set_title('Tiempo de Entrenamiento vs N√∫mero de √Årboles', fontweight='bold')\n",
        "ax2.set_xlabel('N√∫mero de √Årboles')\n",
        "ax2.set_ylabel('Tiempo (segundos)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()  # Ajustar espaciado\n",
        "plt.show()  # Mostrar gr√°ficos\n",
        "\n",
        "# An√°lisis de los resultados\n",
        "print(f\"\\nüìà An√°lisis de resultados:\")\n",
        "print(f\"   ‚Ä¢ Mejor accuracy: {max(accuracies):.4f} con {n_trees_range[accuracies.index(max(accuracies))]} √°rboles\")\n",
        "print(f\"   ‚Ä¢ Tiempo m√°s r√°pido: {min(training_times):.3f}s con {n_trees_range[training_times.index(min(training_times))]} √°rboles\")\n",
        "print(f\"   ‚Ä¢ Tiempo m√°s lento: {max(training_times):.3f}s con {n_trees_range[training_times.index(max(training_times))]} √°rboles\")\n",
        "\n",
        "# Calcular la mejora marginal\n",
        "print(f\"\\nüí° Mejora marginal:\")\n",
        "for i in range(1, len(n_trees_range)):\n",
        "    improvement = accuracies[i] - accuracies[i-1]\n",
        "    print(f\"   ‚Ä¢ De {n_trees_range[i-1]} a {n_trees_range[i]} √°rboles: +{improvement:.4f} accuracy\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
